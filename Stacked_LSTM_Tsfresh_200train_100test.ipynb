{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-NN with Tsfresh\n",
    "#検知したデータを用いてk-NN(100個学習100個推定)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import changefinder\n",
    "import seaborn as sns\n",
    "from tsfresh import extract_features, extract_relevant_features\n",
    "from tsfresh.feature_selection.selection import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'): #GPU使用\n",
    "    \n",
    "    def make_all_ppg_array(data,original_data):#120秒分の脈波を取り出し\n",
    "        time=0\n",
    "        for line in original_data:\n",
    "\n",
    "            if time < 12000 :\n",
    "                data[time] = int(line.split()[3])#何列目とってくるか\n",
    "                time += 1\n",
    "                \n",
    "        return data\n",
    "\n",
    "\n",
    "    def Anomaly_Detection(ppg,answer,label,data_type):\n",
    "\n",
    "        data =np.zeros((12000),int)#データ格納用\n",
    "        det_data = np.zeros((0,det_data_len),int)#検知データ格納用\n",
    "        sub_det_data = np.zeros((det_data_len),int) #切り出した検知データ格納用\n",
    "        det_data_num = 0 #データ数を数えるため\n",
    "\n",
    "\n",
    "        data = make_all_ppg_array(data,ppg)\n",
    "\n",
    "        #確認のためとりあえず描画\n",
    "        plt.plot(data, linestyle = \"dashed\" , label=\"acc\")\n",
    "        plt.xlabel('Epochs',fontsize = 18)\n",
    "        plt.ylabel('Accuracy',fontsize = 18)\n",
    "        plt.legend()\n",
    "        plt.tick_params()\n",
    "        plt.show()\n",
    "\n",
    "        cf = changefinder.ChangeFinder(r=0.01, order=1, smooth=7)#変化点検知にChangeFinder使用\n",
    "\n",
    "        score = np.zeros((12000),int)#異常度格納用\n",
    "        score_num = 0 #score index\n",
    "        for i in data:\n",
    "            sub_score = cf.update(i)\n",
    "            score[score_num] = sub_score\n",
    "            score_num += 1\n",
    "\n",
    "        \n",
    "        det_time=0 #sub_det_data index\n",
    "        wait_F = 0 #ジェスチャ終了時の異常度の山を無視するため\n",
    "        wait_time = 0\n",
    "        for t in range(0,len(score)):\n",
    "            if score[t] > 60 and wait_time == 0 and t>2000: #20秒以降で異常度の閾値を60に\n",
    "                print(f\"record:{t}\")\n",
    "                for k in range(t-100,t+100):\n",
    "                    sub_det_data[det_time] = data[k]\n",
    "                    det_time += 1\n",
    "                det_time = 0\n",
    "                det_data = np.r_[det_data,sub_det_data.reshape(1,-1)] #det_dataに検知したデータを追加\n",
    "                answer_list.append(answer)\n",
    "                label_list.append(label)\n",
    "                wait_time = 1\n",
    "                if data_type == \"train\" :\n",
    "                    det_data_num += 1\n",
    "            elif wait_time != 0 :\n",
    "                wait_time += 1\n",
    "                if wait_time > 500 : #5秒間何もしない\n",
    "                    wait_time = 0\n",
    "\n",
    "        print(det_data) #確認用\n",
    "\n",
    "\n",
    "\n",
    "        # 異常度をプロット\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(score)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(data,'r')\n",
    "        plt.show()\n",
    "        \n",
    "        return det_data,det_data_num\n",
    "\n",
    "    det_data_len = 200 #切り出すデータの長さ\n",
    "    get_data = np.zeros((0,det_data_len),int)\n",
    "    train_data_num = 0\n",
    "    data_num = 0\n",
    "    answer_list = [] #ジェスチャ名格納用\n",
    "    label_list = []  #ジェスチャのラベル格納用\n",
    "    train_label_list = []\n",
    "    test_label_list = []\n",
    "    file_gesture = [\"gu\",\"choki\",\"par\",\"one\",\"three\",\"four\",\"fox\",\"phone\",\"good\",\"koyubi\"]\n",
    "    \n",
    "    for i in range(len(file_gesture)):\n",
    "        for file in glob.glob(f'train_data/{i}*'):\n",
    "            print(file)\n",
    "            with open(file) as ppg :\n",
    "                new_data , data_num = Anomaly_Detection(ppg,file_gesture[i],i,\"train\")\n",
    "                get_data = np.r_[get_data,new_data]\n",
    "                train_data_num += data_num\n",
    "    \n",
    "    for i in range(len(file_gesture)):\n",
    "        print(file)\n",
    "        for file in glob.glob(f'test_data/{i}*'):\n",
    "            with open(file) as ppg :\n",
    "                new_data , data_num = Anomaly_Detection(ppg,file_gesture[i],i,\"test\")\n",
    "                get_data = np.r_[get_data,new_data]\n",
    "        \n",
    "    train_label_list = label_list[:train_data_num]\n",
    "    test_label_list = label_list[train_data_num:]\n",
    "    \n",
    "    np_label_list = np.array(label_list,dtype=int)   #numpy配列に変換\n",
    "    np_train_label_list = np.array(train_label_list,dtype=int)\n",
    "    np_test_label_list = np.array(test_label_list,dtype=int)\n",
    "    \n",
    "    print(answer_list)\n",
    "    print(get_data)\n",
    "    print(get_data.shape)\n",
    "    \n",
    "    #Tsfresh用にDataFrameに変換\n",
    "    get_data_df = pd.DataFrame(get_data)\n",
    "    \n",
    "    Ts_get_data_df = pd.DataFrame({0: get_data_df[:len(get_data)].values.flatten(),\n",
    "                                   1: np.arange(len(get_data)).repeat(get_data_df.shape[1])})\n",
    "    \n",
    "    get_data_features_df = extract_features(Ts_get_data_df, column_id=1)   #特徴量抽出\n",
    "    \n",
    "    get_data_features_df = impute(get_data_features_df)    #Nanなどを補間\n",
    "    \n",
    "    get_data_features_df=get_data_features_df.drop(columns=get_data_features_df.select_dtypes(include='object').columns)     #数値以外の物を除去\n",
    "    \n",
    "    print(get_data_features_df)\n",
    "    print(np_label_list.shape)\n",
    "    selected_get_data_fea = select_features(get_data_features_df,np_label_list)\n",
    "    \n",
    "    get_data_features = selected_get_data_fea.values\n",
    "    \n",
    "    train_data_features = get_data_features[:train_data_num,:]\n",
    "    test_data_features = get_data_features[train_data_num:,:]\n",
    "        \n",
    "    print(train_data_features.shape)\n",
    "    print(test_data_features.shape)\n",
    "\n",
    "    #学習準備\n",
    "    epoch = 289\n",
    "\n",
    "    #k分割交差検証\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    for train_index, eval_index in kf.split(train_data_features,np_train_label_list):\n",
    "        train_x, test_x = train_data_features[train_index], train_data_features[eval_index]\n",
    "        train_t, test_t = np_train_label_list[train_index], np_train_label_list[eval_index]\n",
    "        train_y = np_utils.to_categorical(train_t) # 数値を、位置に変換 [0,1,2] ==> [ [1,0,0],[0,1,0],[0,0,1] ]\n",
    "        test_y = np_utils.to_categorical(test_t) # 数値を、位置に変換 [0,1,2] ==> [ [1,0,0],[0,1,0],[0,0,1] ]\n",
    "        print(train_x)\n",
    "        print(test_x)\n",
    "        print(train_y)\n",
    "        print(test_y)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(LSTM(160,activation=\"tanh\",return_sequences = True , input_shape =(train_x.shape[1])))\n",
    "        model.add(LSTM(160,activation=\"tanh\",return_sequences = True))\n",
    "        model.add(LSTM(160,activation=\"tanh\",return_sequences = True))\n",
    "        model.add(LSTM(160,activation=\"tanh\"))\n",
    "        model.add(Dense(10,activation='softmax'))\n",
    "        # model.add(Activation('softmax'))\n",
    "        rms = RMSprop(lr=2.1453968096740464e-05,clipvalue = 0.29052961802897614)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=rms,metrics =[\"accuracy\"])\n",
    "        #model.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "        #plot_model(model,to_file = 'Stacked_LSTM_model.png')\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath = 'Stacked_val_acc.hdf5',monitor='val_acc',verbose=1,save_best_only=True,save_weights_only=False,mode='max',)\n",
    "\n",
    "        '''\n",
    "        トレーニング\n",
    "        '''\n",
    "        training_data = model.fit(train_x, train_y, epochs=epoch, batch_size=18,validation_data =(test_x,test_y),callbacks = [checkpoint])\n",
    "\n",
    "        '''\n",
    "        学習済みモデルでテストデータを分類する\n",
    "        '''\n",
    "\n",
    "        plt.plot(range(1, epoch+1), training_data.history['acc'], linestyle = \"dashed\" , label=\"acc\")\n",
    "        plt.plot(range(1, epoch+1), training_data.history['val_acc'], label=\"val_acc\")\n",
    "        plt.xlabel('Epochs',fontsize = 18)\n",
    "        plt.ylabel('Accuracy',fontsize = 18)\n",
    "        plt.xlim(0,epoch+11)\n",
    "        plt.ylim(0,1.0)\n",
    "        plt.legend()\n",
    "        plt.tick_params()\n",
    "        plt.show()\n",
    "        plt.savefig('training_with_Stacked_LSTM_200data.png')\n",
    "\n",
    "        del model\n",
    "        model = load_model('Stacked_val_acc.hdf5')\n",
    "        Y_val = model.predict_classes(test_x, batch_size=1)\n",
    "        Y_test = model.predict_classes(test_data_features,batch_size=1)\n",
    "        \n",
    "        '''\n",
    "        結果検証\n",
    "        '''\n",
    "        _, val_index = np.where(test_y > 0) # to_categorical の逆変換\n",
    "        print()\n",
    "        print(confusion_matrix(t_index,Y_val))\n",
    "        print('RESULT Validation')\n",
    "        print(metrics.accuracy_score(t_index,Y_val))\n",
    "\n",
    "        print(confusion_matrix(np_test_label_list,Y_test))\n",
    "        print('RESULT TEST')\n",
    "        print(metrics.accuracy_score(np_test_label_list,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
