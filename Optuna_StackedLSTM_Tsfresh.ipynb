{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-NN with Tsfresh\n",
    "#検知したデータを用いてk-NN(100個学習100個推定)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import changefinder\n",
    "import seaborn as sns\n",
    "from tsfresh import extract_features, extract_relevant_features\n",
    "from tsfresh.feature_selection.selection import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "import glob\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'): #GPU使用\n",
    "    \n",
    "    def make_all_ppg_array(data,original_data):#120秒分の脈波を取り出し\n",
    "        time=0\n",
    "        for line in original_data:\n",
    "\n",
    "            if time < 12000 :\n",
    "                data[time] = int(line.split()[3])#何列目とってくるか\n",
    "                time += 1\n",
    "                \n",
    "        return data\n",
    "\n",
    "\n",
    "    def Anomaly_Detection(ppg,answer,label,data_type):\n",
    "\n",
    "        data =np.zeros((12000),int)#データ格納用\n",
    "        det_data = np.zeros((0,det_data_len),int)#検知データ格納用\n",
    "        sub_det_data = np.zeros((det_data_len),int) #切り出した検知データ格納用\n",
    "        det_data_num = 0 #データ数を数えるため\n",
    "\n",
    "\n",
    "        data = make_all_ppg_array(data,ppg)\n",
    "\n",
    "        #確認のためとりあえず描画\n",
    "        plt.plot(data, linestyle = \"dashed\" , label=\"acc\")\n",
    "        plt.xlabel('Epochs',fontsize = 18)\n",
    "        plt.ylabel('Accuracy',fontsize = 18)\n",
    "        plt.legend()\n",
    "        plt.tick_params()\n",
    "        plt.show()\n",
    "\n",
    "        cf = changefinder.ChangeFinder(r=0.01, order=1, smooth=7)#変化点検知にChangeFinder使用\n",
    "\n",
    "        score = np.zeros((12000),int)#異常度格納用\n",
    "        score_num = 0 #score index\n",
    "        for i in data:\n",
    "            sub_score = cf.update(i)\n",
    "            score[score_num] = sub_score\n",
    "            score_num += 1\n",
    "\n",
    "        \n",
    "        det_time=0 #sub_det_data index\n",
    "        wait_F = 0 #ジェスチャ終了時の異常度の山を無視するため\n",
    "        wait_time = 0\n",
    "        for t in range(0,len(score)):\n",
    "            if score[t] > 60 and wait_time == 0 and t>2000: #20秒以降で異常度の閾値を60に\n",
    "                print(f\"record:{t}\")\n",
    "                for k in range(t-100,t+100):\n",
    "                    sub_det_data[det_time] = data[k]\n",
    "                    det_time += 1\n",
    "                det_time = 0\n",
    "                det_data = np.r_[det_data,sub_det_data.reshape(1,-1)] #det_dataに検知したデータを追加\n",
    "                answer_list.append(answer)\n",
    "                label_list.append(label)\n",
    "                wait_time = 1\n",
    "                if data_type == \"train\" :\n",
    "                    det_data_num += 1\n",
    "            elif wait_time != 0 :\n",
    "                wait_time += 1\n",
    "                if wait_time > 500 : #5秒間何もしない\n",
    "                    wait_time = 0\n",
    "\n",
    "        print(det_data) #確認用\n",
    "\n",
    "\n",
    "\n",
    "        # 異常度をプロット\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(score)\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(data,'r')\n",
    "        plt.show()\n",
    "        \n",
    "        return det_data,det_data_num\n",
    "\n",
    "    det_data_len = 200 #切り出すデータの長さ\n",
    "    get_data = np.zeros((0,det_data_len),int)\n",
    "    train_data_num = 0\n",
    "    data_num = 0\n",
    "    answer_list = [] #ジェスチャ名格納用\n",
    "    label_list = []  #ジェスチャのラベル格納用\n",
    "    train_label_list = []\n",
    "    test_label_list = []\n",
    "    file_gesture = [\"gu\",\"choki\",\"par\",\"one\",\"three\",\"four\",\"fox\",\"phone\",\"good\",\"koyubi\"]\n",
    "    \n",
    "    for i in range(len(file_gesture)):\n",
    "        for file in glob.glob(f'train_data/{i}*'):\n",
    "            print(file)\n",
    "            with open(file) as ppg :\n",
    "                new_data , data_num = Anomaly_Detection(ppg,file_gesture[i],i,\"train\")\n",
    "                get_data = np.r_[get_data,new_data]\n",
    "                train_data_num += data_num\n",
    "    \n",
    "    for i in range(len(file_gesture)):\n",
    "        print(file)\n",
    "        for file in glob.glob(f'test_data/{i}*'):\n",
    "            with open(file) as ppg :\n",
    "                new_data , data_num = Anomaly_Detection(ppg,file_gesture[i],i,\"test\")\n",
    "                get_data = np.r_[get_data,new_data]\n",
    "        \n",
    "    train_label_list = label_list[:train_data_num]\n",
    "    test_label_list = label_list[train_data_num:]\n",
    "    \n",
    "    np_label_list = np.array(label_list,dtype=int)   #numpy配列に変換\n",
    "    np_train_label_list = np.array(train_label_list,dtype=int)\n",
    "    np_test_label_list = np.array(test_label_list,dtype=int)\n",
    "    \n",
    "    print(answer_list)\n",
    "    print(get_data)\n",
    "    print(get_data.shape)\n",
    "    \n",
    "    #Tsfresh用にDataFrameに変換\n",
    "    get_data_df = pd.DataFrame(get_data)\n",
    "    \n",
    "    Ts_get_data_df = pd.DataFrame({0: get_data_df[:len(get_data)].values.flatten(),\n",
    "                                   1: np.arange(len(get_data)).repeat(get_data_df.shape[1])})\n",
    "    \n",
    "    get_data_features_df = extract_features(Ts_get_data_df, column_id=1)   #特徴量抽出\n",
    "    \n",
    "    get_data_features_df = impute(get_data_features_df)    #Nanなどを補間\n",
    "    \n",
    "    get_data_features_df=get_data_features_df.drop(columns=get_data_features_df.select_dtypes(include='object').columns)     #数値以外の物を除去\n",
    "    \n",
    "    print(get_data_features_df)\n",
    "    print(np_label_list.shape)\n",
    "    selected_get_data_fea = select_features(get_data_features_df,np_label_list)\n",
    "    \n",
    "    get_data_features = selected_get_data_fea.values\n",
    "    \n",
    "    train_data_features = get_data_features[:train_data_num,:]\n",
    "    test_data_features = get_data_features[train_data_num:,:]\n",
    "        \n",
    "    print(train_data_features.shape)\n",
    "    print(test_data_features.shape)\n",
    "    \n",
    "    train_onehot = np_utils.to_categorical(np_train_label_list) # 数値を、位置に変換 [0,1,2] ==> [ [1,0,0],[0,1,0],[0,0,1] ]\n",
    "    train_x, test_x, train_t, test_t = train_test_split(train_data_features, train_onehot, train_size=0.8, test_size=0.2,random_state=0,stratify=T) # 訓練とテストで分割\n",
    "    \n",
    "    '''\n",
    "    モデル作成用関数\n",
    "    '''\n",
    "    def create_model(num_layer,activation,mid_units):\n",
    "\n",
    "        model = Sequential()\n",
    "        #model.add(Embedding(100,128,input_length = 200))\n",
    "\n",
    "        model.add(LSTM(mid_units,activation=activation,return_sequences = True , input_shape =(train_x.shape[1],train_x.shape[2])))\n",
    "        for i in range(1,num_layer-1):     #LSTM層を指定された数だけ生成\n",
    "            model.add(LSTM(mid_units,activation=activation,return_sequences = True))\n",
    "\n",
    "        model.add(LSTM(mid_units,activation=activation))\n",
    "        model.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "        return model\n",
    "\n",
    "    ##ハイパーパラメータ最適化##\n",
    "    def objective(trial):\n",
    "        K.clear_session()\n",
    "\n",
    "        #LSTM層の数\n",
    "        num_layer = trial.suggest_int(\"num_layer\",2,5)\n",
    "\n",
    "        #LSTM層のユニット数\n",
    "        mid_units = int(trial.suggest_discrete_uniform(\"mid_units\",16,256,16))\n",
    "\n",
    "        #活性化関数\n",
    "        activation = trial.suggest_categorical(\"activation\",[\"sigmoid\",\"tanh\"])\n",
    "\n",
    "        #最適化関数\n",
    "        optimizer = trial.suggest_categorical('optimizer',['adam','rmsprop'])\n",
    "\n",
    "        #エポック数\n",
    "        epoch = trial.suggest_int('epoch',100,500)\n",
    "\n",
    "        #バッチサイズ\n",
    "        batchsize = trial.suggest_int('batchsize',1,20)\n",
    "\n",
    "        if optimizer == \"adam\" :\n",
    "            #学習率\n",
    "            adam_lr = trial.suggest_loguniform('adam_lr',1e-5,1e-2)\n",
    "            #勾配クリッピング\n",
    "            adam_clipping = trial.suggest_uniform('adam_clipping',0.0,0.5)\n",
    "            optimize = Adam(lr = adam_lr,clipvalue = adam_clipping)\n",
    "        elif optimizer == \"rmsprop\" :\n",
    "            #学習率\n",
    "            rms_lr = trial.suggest_loguniform('rms_lr',1e-5,1e-2)\n",
    "            #勾配クリッピング\n",
    "            rms_clip = trial.suggest_uniform('rms_clip',0.0,0.5)\n",
    "            optimize = RMSprop(lr = rms_lr,clipvalue = rms_clip)\n",
    "\n",
    "        model = create_model(num_layer,activation,mid_units)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimize,metrics =[\"accuracy\"])\n",
    "        \n",
    "        '''\n",
    "        トレーニング\n",
    "        '''\n",
    "        training_data = model.fit(train_x, train_t, epochs=epoch, batch_size=batchsize,validation_data=(test_x,test_t))\n",
    "\n",
    "        '''\n",
    "        学習済みモデルでテストデータで分類する\n",
    "        '''\n",
    "\n",
    "        plt.plot(range(1, epoch+1), training_data.history['acc'], linestyle = \"dashed\" , label=\"acc\")\n",
    "        plt.plot(range(1, epoch+1), training_data.history['val_acc'], label=\"val_acc\")\n",
    "        plt.xlabel('Epochs',fontsize = 18)\n",
    "        plt.ylabel('Accuracy',fontsize = 18)\n",
    "        plt.xlim(0,epoch+1)\n",
    "        plt.ylim(0,1.0)\n",
    "        plt.legend()\n",
    "        plt.tick_params()\n",
    "        plt.show()\n",
    "        plt.savefig('training_with_Stacked_LSTM_200data.png')\n",
    "\n",
    "        '''\n",
    "        結果検証\n",
    "        \n",
    "        val_acc_ave = 0\n",
    "\n",
    "        for i in range(0,epoch):\n",
    "            val_acc_ave += training_data.history['val_acc'][i]\n",
    "\n",
    "        val_acc_ave /= epoch\n",
    "        print(\"average :\"+str(val_acc_ave))\n",
    "\n",
    "        #ハイパーパラメータを求める\n",
    "        return 1 - val_acc_ave\n",
    "        '''\n",
    "        \n",
    "        return 1 - max(training_data.history['val_acc'])\n",
    "\n",
    "\n",
    "    #本体\n",
    "    study = optuna.create_study(study_name='200data_train_Tsfresh',\n",
    "                                storage='sqlite:///200data_train_Tsfresh.db',\n",
    "                                load_if_exists=True)\n",
    "    study.optimize(objective, n_trials = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
